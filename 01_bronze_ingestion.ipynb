{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc89cc97-05b9-4ce0-84c7-17f7cd7577f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base = \"/Workspace/Users/p70madhu@gmail.com/motorsport_project/data/\"\n",
    "\n",
    "# CSV files don't have headers, so we need to specify column names manually\n",
    "lap_columns = ['raceId', 'driverId', 'lap', 'position', 'time', 'milliseconds']\n",
    "\n",
    "lap1 = spark.createDataFrame(pd.read_csv(base+\"lap_times_split_1.csv\", header=None, names=lap_columns))\n",
    "lap2 = spark.createDataFrame(pd.read_csv(base+\"lap_times_split_2.csv\", header=None, names=lap_columns))\n",
    "lap3 = spark.createDataFrame(pd.read_csv(base+\"lap_times_split_3.csv\", header=None, names=lap_columns))\n",
    "lap4 = spark.createDataFrame(pd.read_csv(base+\"lap_times_split_4.csv\", header=None, names=lap_columns))\n",
    "lap5 = spark.createDataFrame(pd.read_csv(base+\"lap_times_split_5.csv\", header=None, names=lap_columns))\n",
    "\n",
    "lap_all = lap1.union(lap2).union(lap3).union(lap4).union(lap5)\n",
    "\n",
    "lap_all.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(\"bronze_lap_times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5779564-5069-422d-afd1-6f769796a612",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ bronze_circuits table created successfully\n✓ bronze_races table created successfully\n"
     ]
    }
   ],
   "source": [
    "# Ingest circuits and races CSV files into bronze tables\n",
    "spark.createDataFrame(pd.read_csv(base+\"circuits.csv\", header=0)).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(\"bronze_circuits\")\n",
    "spark.createDataFrame(pd.read_csv(base+\"races.csv\", header=0)).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(\"bronze_races\")\n",
    "\n",
    "print(\"✓ bronze_circuits table created successfully\")\n",
    "print(\"✓ bronze_races table created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec45f385-fa71-4d65-9928-1b3bffc1c4d8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "outputs": [],
   "source": [
    "# Read JSON files and replace '\\N' with None for proper null handling\n",
    "drivers_df = pd.read_json(base+\"drivers.json\", lines=True).replace('\\\\N', None)\n",
    "constructors_df = pd.read_json(base+\"constructors.json\", lines=True).replace('\\\\N', None)\n",
    "results_df = pd.read_json(base+\"results.json\", lines=True).replace('\\\\N', None)\n",
    "\n",
    "# Convert mixed-type columns to string to avoid Arrow conversion errors\n",
    "results_df = results_df.astype(str)\n",
    "drivers_df = drivers_df.astype(str)\n",
    "constructors_df = constructors_df.astype(str)\n",
    "\n",
    "spark.createDataFrame(drivers_df).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(\"bronze_drivers\")\n",
    "spark.createDataFrame(constructors_df).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(\"bronze_constructors\")\n",
    "spark.createDataFrame(results_df).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(\"bronze_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c3689e-d006-4c6d-86c8-1a94d42c29e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}